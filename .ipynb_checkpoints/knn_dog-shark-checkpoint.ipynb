{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Topic\n",
    "This assignment is about using K-Nearest Neighbors in a image classfication problem. Categories are going to be added, one by one, as work progresses and the methods metrics calculated. Final goal of this work is to somehow improve the benchmark for image recognition basic algorithm, wich has an accuracy equal to 0.56. Some steps are necessary to achieve a better prediction, so the process are:\n",
    "\n",
    "   1. Analysis of Dataset\n",
    "   1. Visual check\n",
    "     - Remove drawing and low resolution images from database\n",
    "     - Make sure all the folders have similar number of pictures, it does not need to be the same, but needs to be close to each other   \n",
    "   1. Pre-processing images\n",
    "     - Convert images from RGB to Grayscale\n",
    "     - Scale all images to have the same size (demand of KNN algorithm)\n",
    "     - Save new Grayscale images in a new folder     \n",
    "   1. Execute Sci-kit learn KNNclassfier loop manually\n",
    "     - Discover best image dimension \n",
    "     - Discover best K number of neighbors     \n",
    "   1. Execute Cross Validation with KNN \n",
    "     - Use k-folds equal to 10\n",
    "     - Get accuracy mean     \n",
    "   1. Results\n",
    "     - Get the results using the best model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Dataset\n",
    "\n",
    "The data was taken from kaggle databases website on the [animals dataset](https://www.kaggle.com/datasets/iamsouravbanerjee/animal-image-dataset-90-different-animals). It is a 90 animal species wich one containing 60 images. Some of the images were low quality or drawing, these species or categories were not used in this work. Images sizes varies, there is no pattern in dimensions. Others data bases were taken from kaggle website, to increment the data used.\n",
    "\n",
    "Images are in png format, all colored, with 3 channels. Sixty images in each of ninety folders, each one containing one animal.\n",
    "\n",
    "Two additionals data set is necessary to improve the results accuracy. [Shark dataset](https://www.kaggle.com/datasets/larusso94/shark-species) and [Dog-Cat dataset](https://www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset)\n",
    "\n",
    "\n",
    "Animals dataset - (https://www.kaggle.com/datasets/iamsouravbanerjee/animal-image-dataset-90-different-animals) \\\n",
    "Shark dataset - (https://www.kaggle.com/datasets/larusso94/shark-species) \\\n",
    "Cats-Dogs dataset - (https://www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import PIL\n",
    "import umap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from PIL import Image\n",
    "from statistics import mean\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visual Check\n",
    "\n",
    "\n",
    "\n",
    "Visual check consists in looking the pictures that will be used and search for low quality ones. The **FOLDERS** cointaning images that will be used in the process needs to be added in 'animal-images' folder. The word folder is highlighted due to its importance. The name of each label is taken from the folder name where the image is located and they are organized in alphabetic orders. Images from folder 'dog' are going to be labeled as dog, the same for the other species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = glob.glob('animal-images/*/*.jpg')    #save all COMPLETE directorys path at 'images_path' for every image inside 'animal-images'\n",
    "#os.mkdir('bw-pic')                                 #create a folder for grayscale \n",
    "#images_path2 = glob.glob('bw-pic/*.jpg')            \n",
    "images_path3 = glob.glob('to_predict/*/*.jpg')      #dataset with new images to be predicted  (SHARK AND DOG) \n",
    "images_path6 = glob.glob('to_predict2/*/*.jpg')     #dataset with new images to be predicted  (SHARK, DOG AND CAT)\n",
    "images_path4 = glob.glob('animal-images4/*/*.jpg')  #dataset with images to train the model (SHARK AND DOG IMAGES) \n",
    "images_path5 = glob.glob('animal-images1/*/*.jpg')  #dataset with images to train the model (SHARK, DOG AND CAT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Pre-processing images\n",
    "\n",
    "KNN is a simple method of machine learning, but it does not \"learn\" parameters, it only takes the closest data possible to what is being evaluated. Due to this method caracteristics, and the lack of memory of my computer, I converted all images into grayscale ones, as it will divide the number of integers saved in a dataframe by 3. After the conclusion of this work, an extra session will be included in the end to compare results from Grayscale images to RGB ones.\n",
    "\n",
    "Scaling images can be a determinant step of the process. This can be done by many ways, keeping the width x height ratio, or, just squash the dimensions. Either cases can be successful, but of course, preserve the aspect ratio is perfect scenario, but not always necessary. As it takes a long time to keep the aspect ratio, I am going to do the simpler way.\n",
    "\n",
    "The method 'get_image' was created to read the image file, save the folder name where the image is located as an object, convert it to grayscale, flatten the pixel matrix and, by the end, save it in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a black and white folder with all colored pictures\n",
    "class get_image:\n",
    "    def __init__(self, dsize_x, dsize_y, images_path = images_path ):\n",
    "        dsize = (dsize_x, dsize_y)                                   #image dimensions\n",
    "        i=0\n",
    "        self.labels=[]                                               #list of labels of each image\n",
    "        pix_img=list()                                               #list of pixel of each image\n",
    "        self.img_height = []\n",
    "        self.img_width = []                                               \n",
    "        for image in images_path:\n",
    "            img = cv2.imread(image) \n",
    "            self.img_height.append(img.shape[1])\n",
    "            self.img_width.append(img.shape[0])\n",
    "            folder=os.path.split(os.path.dirname(image))[-1]         #function to discover in wich folder it is located\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)         #convert image from rgb to grayscale\n",
    "            gray_img = cv2.resize(gray_img, dsize)                   #resize grayscale image\n",
    "            gray_img_pix = gray_img.flatten()                        #turn image matrix into a single line of pixels\n",
    "            pix_img.append(gray_img_pix)                              \n",
    "            self.labels.append(folder)\n",
    "            i=i+1\n",
    "\n",
    "        ### function to convert labels in numbers.###\n",
    "        uniqueWord = [];\n",
    "        self.num_lab=[]\n",
    "        for elt in self.labels:\n",
    "            if elt not in uniqueWord:\n",
    "                uniqueWord.append(elt);\n",
    "        for word in self.labels:\n",
    "            self.num_lab.append(uniqueWord.index(word) + 1)\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(self.num_lab, columns = ['labels'])        #save numeric labels as a data frame shape(-1,1)\n",
    "        self.y= df['labels']                                         #categories column\n",
    "        self.X=pd.DataFrame(pix_img)                                 #matrix of random variable\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For images, the EDA consists in checking the image sizes to know if there will be a lot of distortion. An histogram is made for width and another for height. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean height of the images is 743.8511749347258\n",
      "mean width of the images is 572.5352480417755\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = get_image(60, 60, images_path4)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2);\n",
    "ax1.hist(data.img_height, 15, color ='green', alpha = 0.7) ; \n",
    "ax1.set_title(\"Image height\");\n",
    "ax2.hist(data.img_width, 15, color ='green', alpha = 0.7);\n",
    "ax2.set_title(\"Image width\");\n",
    "\n",
    "print('mean height of the images is', mean(data.img_height))\n",
    "print('mean width of the images is', mean(data.img_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute Sci-kit learn KNN classfier loop manually\n",
    "This is a simple loop to get accuracy score predictions, where the n numbers of neighbors are incresing one in each loop. KNN is a non linear response algorithm, so it does not have a pattern. Increasing the number of neighbors not necessarily will increase accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for k equal to 1: 0.5454545454545454\n",
      "for k equal to 2: 0.7272727272727273\n",
      "for k equal to 3: 0.6103896103896104\n",
      "for k equal to 4: 0.6883116883116883\n",
      "for k equal to 5: 0.6233766233766234\n"
     ]
    }
   ],
   "source": [
    "data = get_image(740, 570, images_path4)\n",
    "for i in range(1,6):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn = knn.fit(data.X_train, data.y_train)\n",
    "    knn.score(data.X_test, data.y_test)\n",
    "    print('for k equal to %d:' % i, knn.score(data.X_test, data.y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execute Cross Validation with KNN\n",
    "In order to determine wether the dimensions of the images in the data set will cause a substantial improvement, a more complex loop is made, where the images sides are equal and it goes from 30 to 300 pixels. The number of points analyzed are 900 and 90000 respectively. That means, if it does not change the accuracy, it is better be using smaller images, for computational reasons. If the data has large number of images, bigger images like 300 pixels side will demand large computational power as well. Inside loop computes k-fold cross validation score, for each number of neighbors, from 1 to 20.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image of side 30 \n",
      "image of side 60 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "score_list=list()\n",
    "lst_of_lsts=[]\n",
    "for j in range(30,301,30)  :\n",
    "    data = get_image(j,j, images_path4 )                                              #get images with jxj dimensions from 'images_path' folder\n",
    "    score_int=list()                                                                  #start a new list every loop to include the accuracy mean by knn neighbors\n",
    "    print('image of side %d ' %j)\n",
    "    for i in range(1,21):\n",
    "        knn = KNeighborsClassifier(n_neighbors = i)                                   #instance of knn\n",
    "        scores = cross_val_score(knn, data.X, data.y, cv = 15, scoring = 'accuracy')  #calculate knn accuracy score, for kfolds = 15\n",
    "        score_int.append(scores.mean())                                               #save accuracy mean in a list\n",
    "        if i == 20:\n",
    "            lst_of_lsts.append(score_int)                                             #append the complete list, at last step, in lst_of_lsts\n",
    "            score_list.append(max(score_int))                                  \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve data visualization, used T.plot to represent all images size by each color. Starting from the smallest until the greatest one. This plot will help to choose the best image dimension, according to the accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2=pd.DataFrame(lst_of_lsts, index=pd.RangeIndex(start=1, stop=len(lst_of_lsts)+1, step=1), columns=range(1,21,1))\n",
    "\n",
    "\n",
    "pltt = df2.T.plot(figsize=(7,6), xticks=range(1,21,1))   \n",
    "pltt.set_ylabel('Accuracy mean', fontsize=12)   \n",
    "pltt.set_xlabel('Number of Nearest Neighbors', fontsize=12, labelpad=1)\n",
    "pltt.xaxis.set_major_locator(MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that there is not a big improvement when using greater dimensions images. So, the analysis will be done with 60x60 images, to avoid spending a lot of time, and to explore the possibility to include more images in the dataset.\n",
    "\n",
    "About cross validation number of folds, it will be used 10 Kfold. To check wich n neighbors is the best to this situation, the GridSearchCV hyperparameter tuning method will be used. This functions iterates over the 'n_neighbors', from 1 to 15 and the metrics that will be used with euclidean and manhattan distance. Mean accuracy and stardard deviation will be put in a table with the number of neighbors used in KNN algorithm.\n",
    "\n",
    "- Rows 1 to 14 are the model mean test score using euclidean distance.\n",
    "- Rows 15 to 29 are the model mean test score using manhattan distance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_image(60, 60, images_path4)\n",
    "\n",
    "clf = GridSearchCV( KNeighborsClassifier(),{'n_neighbors' : range(1,16), \n",
    "                                            'metric' : [ 'euclidean', 'manhattan']},\n",
    "                                            cv = KFold(n_splits=10, random_state=1, shuffle=True), return_train_score=False)\n",
    "clf.fit(data.X, data.y)\n",
    "clf.cv_results_\n",
    "df = pd.DataFrame(clf.cv_results_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['param_n_neighbors', 'mean_test_score', 'std_test_score']])        \n",
    "print('min and max score values are', min(df['mean_test_score']), max(df['mean_test_score']))\n",
    "print('min and max std values are', min(df['std_test_score']), max(df['std_test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Euclidean Distance\n",
    "    - min and max score values are 0.5849 and 0.7207\n",
    "    - the standard deviation of these scores are 0.071129 and 0.097129 respectively\n",
    "\n",
    " - Manhattan distance\n",
    "    - min and max score values are 0.642308 and 0.741835\n",
    "    - the standard deviation of these scores are 0.1105 and 0.0920 respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean test score is between the range of 0.76 and 0.88. Looking at the previous table, it is possible to see that the best accuracy is obtained with K neighbors equal to 2, for both metrics. To be sure of that, a list is made to append the number of neighbors that will be optmizing the model in each image size. While from the cross validation session we had determined that the image dimensions will be 60x60, the best k neighbors will be determined by the seccond value in the list \"mean_max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_max = []                                                  #number of neighborhood for max accuracy in each image dimension\n",
    "for i in range(len(lst_of_lsts)):\n",
    "    max_index = lst_of_lsts[i].index(max(lst_of_lsts[i]))\n",
    "    mean_max.append(max_index)\n",
    "\n",
    "mean_max = [x+1 for x in mean_max]\n",
    "print('The best K number of neighbors for the dimension size 60x60 is',mean_max[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "After deep analysis in the data and models, the number of nearest neighbors was set to 2, and the images dimensions were 60x60. A prediction in this terms led to a mean test score using cross validation equal to 0.88. Now I am going to check if predictions using new data are going to lead to a similar result. \n",
    "\n",
    "The new folder cointaining the images to be predicted has 20 images from each label. Each image must be inside a internal folder named with the label to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_image(60, 60, images_path4)\n",
    "data2 = get_image(60,60, images_path3)\n",
    "h=0\n",
    "knn = KNeighborsClassifier(n_neighbors = 2, metric = 'manhattan' ).fit(data.X, data.y)\n",
    "for i in range(len(data2.y.tolist())):\n",
    "    if knn.predict(data2.X)[i] == data2.y.tolist()[i]:\n",
    "        h=h+1\n",
    "        \n",
    "acc = h/len(data2.y.tolist())\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using new images, from other databases, the accuracy score has fallen 0.2 points to 0.74, wich is a bad result comparing to human accuracy (about 0.95). On the other hand, it had a slightly improvement when comparing to image recognition KNN accuracy benchmark for cat-dog databases(about 0.54). \n",
    "\n",
    "The improvement must be reached due the used images had differents backgrounds. For classification of 2 labels, the animals analyzed were Dog and Sharks. Dog pictures has multiple backgrounds, for sharks, almost every image was taken under water.\n",
    "So, similar backgrounds tends to have smaller distance than differents ones.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(metrics.classification_report(data2.y, knn.predict(data2.X)))\n",
    "print(pd.crosstab(data2.y, knn.predict(data2.X), margins  = True, rownames=['real'], colnames=['predito'] ))\n",
    "mat = confusion_matrix(data2.y, knn.predict(data2.X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With new image data set, the accuracy over the prediction of all images is equal to 0.74."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Images back ground in many ways interfere in the results for KNN. There are other academic works that improve the KNN accuracy for image recognition. Ones suggests that background needs to be removed. I do think that for species classification, KNN is not the best model to be implemented, but it can be more effective when comparing aquatic to land animals. I would suggest using Convolutional neural network as a more effective model for this kind of problem. For datasets like MNIST, KNN would work well enough, as the images are standardized, with numbers in same size, wich makes distance comparition much more effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA - multiclass classification\n",
    "\n",
    "Using more categories or animals species, we will probably have more difficult to correctly classify them. The way the pictures were taken was not standardized for all the pictures, that is why a bigger data set is necessary in order to keep a reasonable result. \n",
    "\n",
    "The algorithm could be implemented for many categories. Said so, I decided to make a multiclass classification, wich labels are Cat, Dog and Shark (1,2 and 3), using same protocol as binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_image(60,60, images_path5)            #train model (CAT, DOG AND SHARK)\n",
    "data2 = get_image(60,60, images_path6)           #to predict (CAT, DOG AND SHARK)\n",
    "\n",
    "score_list=list()\n",
    "lst_of_lsts=[]\n",
    "for j in [30, 120, 210, 300]  :\n",
    "    data = get_image(j,j, images_path5 )                                              #get images with jxj dimensions\n",
    "    score_int=list()                                                                  #start a new list every loop to include the accuracy mean by knn neighbors\n",
    "    print('image of side %d ' %j)\n",
    "    for i in range(1,21):\n",
    "        knn = KNeighborsClassifier(n_neighbors = i)                                   #instance of knn\n",
    "        scores = cross_val_score(knn, data.X, data.y, cv = 15, scoring = 'accuracy')  #calculate knn accuracy score, for kfolds = 15\n",
    "        score_int.append(scores.mean())                                               #save accuracy mean in a list\n",
    "        if i == 20:\n",
    "            lst_of_lsts.append(score_int)                                             #append the complete list, at last step, in lst_of_lsts\n",
    "            score_list.append(max(score_int))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2=pd.DataFrame(lst_of_lsts, index=pd.RangeIndex(start=1, stop=len(lst_of_lsts)+1, step=1), columns=range(1,21,1))\n",
    "\n",
    "\n",
    "pltt = df2.T.plot(figsize=(7,6), xticks=range(1,21,1))   \n",
    "pltt.set_ylabel('Accuracy mean', fontsize=12)   \n",
    "pltt.set_xlabel('Number of Nearest Neighbors', fontsize=12, labelpad=1)\n",
    "pltt.xaxis.set_major_locator(MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_image(60,60, images_path5)            #train model (CHIPANZEE, DOG AND SHARK)\n",
    "data2 = get_image(60,60, images_path6)           #to predict\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 2).fit(data.X, data.y)\n",
    "knn.predict(data2.X)\n",
    "\n",
    "h=0\n",
    "for i in range(len(data2.y.tolist())):\n",
    "    if knn.predict(data2.X)[i] == data2.y.tolist()[i]:\n",
    "        h=h+1\n",
    "    \n",
    "acc2=h/len(data2.y.tolist())\n",
    "print(acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_image(60,60, images_path5)            #train model (CHIPANZEE, DOG AND SHARK)\n",
    "data2 = get_image(60,60, images_path6)           #to predict\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 2).fit(data.X, data.y)\n",
    "knn.predict(data2.X)\n",
    "print(metrics.classification_report(data2.y, knn.predict(data2.X)))\n",
    "print(pd.crosstab(data2.y, knn.predict(data2.X), margins  = True, rownames=['real'], colnames=['predito'] ))\n",
    "mat = confusion_matrix(data2.y, knn.predict(data2.X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dogs and cats are easy to be mistaken as they format are similar, and the background that they use to appear at is similar in many ways and tottaly different from shark images background. That can cause a reduction in accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
